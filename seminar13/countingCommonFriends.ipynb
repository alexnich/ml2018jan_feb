{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.1.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.13 (default, Dec 20 2016 23:05:08)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "execfile(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x110ef3890>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graphPath = \"/Users/pritykovskaya/Desktop/EY/graph/part-00000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = spark.read.format(\"csv\").option(\"delimiter\", \"\\t\")\\\n",
    "    .load(graphPath).withColumnRenamed(\"_c0\", \"user\").withColumnRenamed(\"_c1\", \"friendsString\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "| user|       friendsString|\n",
      "+-----+--------------------+\n",
      "|  423|{(76034,0),(91316...|\n",
      "| 1431|{(25244,0),(37528...|\n",
      "| 2039|{(1378599,0),(502...|\n",
      "| 4999|{(188333,0),(1999...|\n",
      "| 5159|{(168092,0),(1866...|\n",
      "| 5975|{(9800,0),(14904,...|\n",
      "| 6519|{(222812,0),(2726...|\n",
      "| 7159|{(40034,0),(33552...|\n",
      "| 7735|{(567383,0),(1339...|\n",
      "|10119|{(2657110,0),(345...|\n",
      "|10311|{(225212,1024),(3...|\n",
      "|13719|{(113697,0),(3491...|\n",
      "|14951|      {(27292725,0)}|\n",
      "|15815|{(939274,0),(1910...|\n",
      "|17111|{(205443,1024),(2...|\n",
      "|18487|{(3371513,0),(709...|\n",
      "|20199|{(160621,0),(2896...|\n",
      "|20359|{(351400,0),(4019...|\n",
      "|20567|{(74837,0),(86428...|\n",
      "|23735|{(164356,0),(3772...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import abs, col, explode, collect_list, sort_array, size, split\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def cutStartEndBrackets(s):\n",
    "    return s[2:-2]\n",
    "\n",
    "cutStartEndBracketsUDF = udf(cutStartEndBrackets, StringType())\n",
    "\n",
    "userFriend = \\\n",
    "    data.select(col(\"user\"), split(cutStartEndBracketsUDF(col(\"friendsString\")), \"\\),\\(\").alias(\"friendsMasks\"))\\\n",
    "    .withColumn(\"friendMask\", explode('friendsMasks'))\\\n",
    "    .withColumn(\"friend\", split(col(\"friendMask\"), \",\")[0])\\\n",
    "    .select(col(\"user\").cast(\"integer\"), col(\"friend\").cast(\"integer\"))\n",
    "\n",
    "usersWithCommonFriend = userFriend\\\n",
    "    .groupBy(\"friend\")\\\n",
    "    .agg(collect_list(\"user\").alias(\"users\"))\\\n",
    "    .withColumn(\"usersWithCommonFriend\", sort_array(\"users\")) \\\n",
    "    .where(size(col(\"usersWithCommonFriend\")) >= 2) \\\n",
    "    .select(col(\"usersWithCommonFriend\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- usersWithCommonFriend: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersWithCommonFriend.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def orderedPairsWithCommonFriend(usersWithCommonFriend):\n",
    "    pairs = []\n",
    "    for user1Index in range(0, len(usersWithCommonFriend)):\n",
    "         for user2Index in range(user1Index + 1, len(usersWithCommonFriend)):\n",
    "             pairs.append((usersWithCommonFriend[user1Index], usersWithCommonFriend[user2Index]))\n",
    "    return pairs\n",
    "\n",
    "schema = ArrayType(StructType([\n",
    "    StructField(\"user1\", IntegerType(), False),\n",
    "    StructField(\"user2\", IntegerType(), False)\n",
    "]))\n",
    "         \n",
    "orderedPairsWithCommonFriendUdf = udf(orderedPairsWithCommonFriend, schema)\n",
    "        \n",
    "commonFriendsCounts = usersWithCommonFriend\\\n",
    "        .select(orderedPairsWithCommonFriendUdf(\"usersWithCommonFriend\").alias(\"orderedPairsWithCommonFriend\"))\\\n",
    "        .withColumn(\"orderedPairWithCommonFriend\", explode(\"orderedPairsWithCommonFriend\")) \\\n",
    "        .groupBy(\"orderedPairWithCommonFriend\")\\\n",
    "        .count()\\\n",
    "        .select(col(\"count\"),\\\n",
    "                col(\"orderedPairWithCommonFriend.user1\").alias(\"user1\"),\\\n",
    "                col(\"orderedPairWithCommonFriend.user2\").alias(\"user2\"))\\\n",
    "        .write.parquet(\"commonFriendsCounts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, row_number, concat_ws\n",
    "from pyspark.sql import Window\n",
    "\n",
    "window = Window.partitionBy(col(\"user1\")).orderBy(col(\"count\").desc())\n",
    "\n",
    "spark.read.parquet(\"commonFriendsCounts\")\\\n",
    "    .withColumn(\"row_number\", row_number().over(window))\\\n",
    "    .filter(col(\"row_number\") < 100)\\\n",
    "    .groupBy(col(\"user1\"))\\\n",
    "    .agg(collect_list(col(\"user2\").cast(\"string\")).alias(\"candidates\"))\\\n",
    "    .withColumn(\"candidatesStr\", concat_ws(\" \", col(\"candidates\")))\\\n",
    "    .select(col(\"user1\").cast(\"string\"), col(\"candidatesStr\"))\\\n",
    "    .repartition(1)\\\n",
    "    .write.csv('candidates.csv', sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
