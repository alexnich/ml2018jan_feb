{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Орг. вопросы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Настройка среды"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кода написан для Python 3.6.0.\n",
    "Скорее всего, с другими версиями Python проблем не возникнет, но может понадобиться немного адаптировать код.\n",
    "\n",
    "Проще всего установить Anaconda: https://www.continuum.io/downloads <br/>\n",
    "Там уже есть почти все нужные нам библиотеки.\n",
    "\n",
    "Можно все установить и вручную. В этом случае рекомендую посмотреть на pyenv. Он позволяет держать на компьютере несколько виртуальных сред с разными версиями python и набором библиотек.<br/>\n",
    "Здесь есть неплохой мануал по использованию: http://www.chriskrycho.com/2015/a-modern-python-development-toolchain.html <br/>\n",
    "Мануал для Mac, но если использовать apt-get вместо brew, все должно заработать и на linux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот список библиотек, которые понадобятся (если у вас Anaconda, все кроме pymorphy уже должно стоять):\n",
    "```\n",
    "numpy scipy pandas jupyter matplotlib scikit-learn nltk pymorphy2[fast] pymorphy2-dicts-ru\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Извлечение признаков из текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом семинаре мы рассмотрим подходы и доступные библиотеки, позволяющие собирать признаки из текстов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распространенная библиотека для обработки текстов. Содержит алгоритмы, корпусы, обученные модели. <br/>\n",
    "http://www.nltk.org <br/>\n",
    "Есть книга по обработке текста с использованием NLTK: http://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместе с nltk идет набор данных, который не скачивается по-умолчанию. <br/>\n",
    "Для выполнения кода, приведенного ниже может понадобиться скачать дополнительные файлы. Для этого запустите код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download() # Откроется обычное GUI-окно. Закройте его, чтобы продолжить выполнение ноутбука."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиение текста на токены (слова, пунктуация, числа, url, телефоны, ...)\n",
    "\n",
    "http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr. Smith!', 'Glad to see you.', '$3.55']\n",
      "['Mr.', 'Smith', '!', 'Glad', 'to', 'see', 'you', '.', '$', '3.55']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"Mr. Smith! Glad to see you. $3.55\"\n",
    "\n",
    "print(sent_tokenize(text))\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно следить, чтобы токенизация правильно работала на ваших текстах. <br/>Например, на текстах из twitter стандартный токенизатор может работать плохо:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '@', 'username', '!', '!', '!', '!', '!', ':', ')', 'http', ':', '//alink.com/page', '#', 'part', '#', 'hashtag']\n"
     ]
    }
   ],
   "source": [
    "text = \"hello @username!!!!! :) http://alink.com/page#part #hashtag\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " '@username',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " ':)',\n",
       " 'http://alink.com/page#part',\n",
       " '#hashtag']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "tokenizer = TweetTokenizer(reduce_len=True)\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда помогает объединить несколько токенов в один. Например, устойчивые выражения или конструкции вида $100, 10x15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In', 'a_little', 'or', 'a_little_bit', 'or', 'a_lot', 'in_spite_of']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "tokenizer = MWETokenizer([('a', 'little'), ('a', 'little', 'bit'), ('a', 'lot')])\n",
    "tokenizer.add_mwe(('in', 'spite', 'of'))\n",
    "tokenizer.tokenize(word_tokenize('In a little or a little bit or a lot in spite of'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Даже такая задача, как разбиение на токены - не тривиальная. <br/>\n",
    "Еще сильней она усложняется, если в тексте встречаются опечатки, или он получен с помощью OCR.\n",
    "\n",
    "В некоторых случаях правильное деление на токены можно выполнить только с учетом знаний о морфологии, синтаксисе. В этом случае можно порождать разные гипотезы разбиения на токены (граф линейного деления), и отбирать их на следующем этапе анализа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Стоп-слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Списки стоп-слов используются для того, чтобы игнорировать слова, которые не помогают в решении задачи.\n",
    "\n",
    "Это могут быть конструкции, которые не несут самостоятельного смысла в предложении. Для классификации и кластеризации могут быть бесполезны высокочастотные слова, которые есть почти в каждом тексте.\n",
    "\n",
    "Фильтрация стоп-слов иногда помогает повысить качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "и\n",
      "в\n",
      "во\n",
      "не\n",
      "что\n",
      "он\n",
      "на\n",
      "я\n",
      "с\n",
      "со\n",
      "как\n",
      "а\n",
      "то\n",
      "все\n",
      "она\n",
      "так\n",
      "его\n",
      "но\n",
      "да\n",
      "ты\n",
      "к\n",
      "у\n",
      "же\n",
      "вы\n",
      "за\n",
      "бы\n",
      "по\n",
      "только\n",
      "ее\n",
      "мне\n",
      "было\n",
      "вот\n",
      "от\n",
      "меня\n",
      "еще\n",
      "нет\n",
      "о\n",
      "из\n",
      "ему\n",
      "теперь\n",
      "когда\n",
      "даже\n",
      "ну\n",
      "вдруг\n",
      "ли\n",
      "если\n",
      "уже\n",
      "или\n",
      "ни\n",
      "быть\n",
      "был\n",
      "него\n",
      "до\n",
      "вас\n",
      "нибудь\n",
      "опять\n",
      "уж\n",
      "вам\n",
      "ведь\n",
      "там\n",
      "потом\n",
      "себя\n",
      "ничего\n",
      "ей\n",
      "может\n",
      "они\n",
      "тут\n",
      "где\n",
      "есть\n",
      "надо\n",
      "ней\n",
      "для\n",
      "мы\n",
      "тебя\n",
      "их\n",
      "чем\n",
      "была\n",
      "сам\n",
      "чтоб\n",
      "без\n",
      "будто\n",
      "чего\n",
      "раз\n",
      "тоже\n",
      "себе\n",
      "под\n",
      "будет\n",
      "ж\n",
      "тогда\n",
      "кто\n",
      "этот\n",
      "того\n",
      "потому\n",
      "этого\n",
      "какой\n",
      "совсем\n",
      "ним\n",
      "здесь\n",
      "этом\n",
      "один\n",
      "почти\n",
      "мой\n",
      "тем\n",
      "чтобы\n",
      "нее\n",
      "сейчас\n",
      "были\n",
      "куда\n",
      "зачем\n",
      "всех\n",
      "никогда\n",
      "можно\n",
      "при\n",
      "наконец\n",
      "два\n",
      "об\n",
      "другой\n",
      "хоть\n",
      "после\n",
      "над\n",
      "больше\n",
      "тот\n",
      "через\n",
      "эти\n",
      "нас\n",
      "про\n",
      "всего\n",
      "них\n",
      "какая\n",
      "много\n",
      "разве\n",
      "три\n",
      "эту\n",
      "моя\n",
      "впрочем\n",
      "хорошо\n",
      "свою\n",
      "этой\n",
      "перед\n",
      "иногда\n",
      "лучше\n",
      "чуть\n",
      "том\n",
      "нельзя\n",
      "такой\n",
      "им\n",
      "более\n",
      "всегда\n",
      "конечно\n",
      "всю\n",
      "между\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "for w in stopwords.words(\"russian\"):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Стэмминг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "run\n",
      "meet\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "print(ps.stem(\"cats\"))\n",
    "print(ps.stem(\"running\"))\n",
    "print(ps.stem(\"meeting\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "running\n",
      "meeting\n",
      "better\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"running\"))\n",
    "print(lemmatizer.lemmatize(\"meeting\"))\n",
    "print(lemmatizer.lemmatize(\"better\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS-тэггинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of Speech тэггинг позволяет разметить токены тегами частей речи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She', 'saw', 'the', 'man', 'with', 'a', 'telescope']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"She saw the man with a telescope\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('She', 'PRP'),\n",
       " ('saw', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('man', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('telescope', 'NN')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "# Получить список доступных тэгов можно с помощью команды:\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часть речи может зависеть от контекста, в котором употреблено слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('fly', 'NN'), ('to', 'TO'), ('London', 'NNP')]\n",
      "[('I', 'PRP'), ('fly', 'VBP'), ('to', 'TO'), ('London', 'NNP')]\n",
      "[('I', 'PRP'), ('opened', 'VBD'), ('a', 'DT'), ('window', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(nltk.word_tokenize(\"A fly to London\")))\n",
    "print(nltk.pos_tag(nltk.word_tokenize(\"I fly to London\")))\n",
    "print(nltk.pos_tag(nltk.word_tokenize(\"I opened a window\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('blablablowed', 'VBD'), ('the', 'DT'), ('window', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(nltk.word_tokenize(\"I blablablowed the window\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что если назначать часть речи каждому слову наивно - наиболее частотную часть речи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS')]\n"
     ]
    }
   ],
   "source": [
    "# Воспользуемся brown корпусом, входящим в состав NLTK.\n",
    "# Он размечен тегами частей речи. Посмотрим на несколько первых токенов:\n",
    "from nltk.corpus import brown\n",
    "print(brown.tagged_words(categories='news')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PPSS'), ('fly', 'NN'), ('to', 'TO'), ('London', 'NP')]\n",
      "[('A', 'AT'), ('fly', 'NN'), ('to', 'TO'), ('London', 'NP')]\n"
     ]
    }
   ],
   "source": [
    "# Теперь обучим на корпусе UnigramTagger - теггер, который выбирает для каждого слова ту часть речи, \n",
    "# с которой оно чаще всего встречалось в обучающем корпусе.\n",
    "from nltk.tag import UnigramTagger\n",
    "tagger = UnigramTagger(brown.tagged_sents(categories='news')[:3000]) # Возьмем первые 3000 токенов\n",
    "print(tagger.tag(nltk.word_tokenize(\"I fly to London\")))\n",
    "print(tagger.tag(nltk.word_tokenize(\"A fly to London\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7878104465976167"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверим качество на отложенном фрагменте brown-корпуса.\n",
    "tagger.evaluate(brown.tagged_sents(categories='news')[3000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, что цифра 0.78 достаточно высокая для такого наивного подхода.<br/>\n",
    "Но если задуматься, то это значит, что часть речи каждого пятого слова была определена."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Морфологический анализ - Pymorphy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для русского и украинского языка вы можете использовать PyMorphy<br/>\n",
    "https://pymorphy2.readthedocs.io/en/latest/user/guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='стали', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='стать', score=0.984662, methods_stack=((<DictionaryAnalyzer>, 'стали', 904, 4),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 1),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,datv'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 2),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,loct'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 5),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,nomn'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 6),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,accs'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 9),))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "# Посмотрим варианты морфологического разбора слова \"стали\":\n",
    "morph.parse('стали')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB,perf,intr plur,past,indc\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "# Получим наиболее вероятный тег и часть речи:\n",
    "tag = morph.parse('стали')[0].tag\n",
    "print(tag)\n",
    "print(tag.POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'думать'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Приведение к нормальной форме:\n",
    "morph.parse('думающему')[0].normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другие инструменты для работы с русским языком (POS-тэггинг, синтаксический анализ) доступны на странице<br/>\n",
    "http://corpus.leeds.ac.uk/mocky/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регулярные выражения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью регулярных выражений можно осуществлять предобработку текстов, строить признаки-шаблоны на слова и подстроки текста и многое другое.<br/>\n",
    "Документация для python:<br/>\n",
    "https://docs.python.org/3.6/howto/regex.html <br/>\n",
    "https://docs.python.org/3.6/library/re.html#module-re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хочется обратить внимание на два полезных момента.<br/>\n",
    "1) re.X позволяет использовать пробельные символы и делать комментарии в ваших регулярных выражениях. Это очень удобно!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(5, 11), match='1.1232'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "a = re.compile(r\"\"\"\\d +  # the integral part\n",
    "                   \\.    # the decimal point\n",
    "                   \\d *  # some fractional digits\"\"\", re.X)\n",
    "b = re.compile(r\"\\d+\\.\\d*\")\n",
    "a.search(\"asdf 1.1232\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Существуют не жадные версии операторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(5, 7), match='1.'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Не жадные версии операторов: +? *? ??\n",
    "c = re.compile(r\"\\d+\\.\\d*?\")\n",
    "c.search(\"asdf 1.1232\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
