# Домашнее задание

Задания присылать на почту **ey.ml.course.hw@gmail.com** с темой "[HW 4 - Crawling Data] Фамилия Имя"


Дедлайн: 19:00 12 февраля.
Если сделали задание, потом решили что-то доделать/исправить - новое письмо отправляйте в ту же цепочку писем (ответом на предыдущее)

Присылая задание - отправляйте ссылку на него в вашем репозитории на github.com и на всякий случай прикрепляйте файлы к письму.

Если не получается, просто прикрепляйте файлы к письму без ссылки на гитхаб.

Если задание не получится довести до конца - присылайте частичное решение (можно даже не заработавший код)


## Анализ тональности отзывов: часть 1 - постановка задачи и сбор данных

В этом задании мы начинаем проект, который продолжим делать через одну неделю (сразу после занятия про анализ текстов). 

Представьте, что вам нужно реализовать алгоритм анализа тональности отзывов о банках. Задача изначально стоит в самом неопределенном виде: научиться отличать позитивные отзывы и негативные. Нет ни более четкой постановки, ни данных, но нужно как-то продемонстрировать возможность решения задачи, потратив на это не слишком много времени.

### Задание 1
Формализуйте постановку задачи, ответив на вопросы: 
* какая задача будет решаться (классификация/регрессия/кластеризация/еще что-то); 
* какими будут целевые значения (например, классы в задаче классификации) - почему именно такими, как прогнозы будут показываться в демонстрации (например: каким-то числом, текстом, цветом); 
* как измерять качество, чтобы было более-менее интуитивно понятно, высокое оно или не очень, 
* на каких отзывах должна работать демонстрация (язык, длина, наличие ошибок и сленга в тексте). 

Возможно вам потребуется частично справиться со следующим пунктом задания, чтобы постановка задачи была совместима с данными, которые вы можете достать. 

### Задание 2
Соберите данные: тексты отзывов и какую-то разметку (желательно, не менее 5000 примеров), позволяющую судить о том, какие отзывы позитивные, а какие нет. Например, можно попробовать найти готовую выборку, либо распарсить тексты с каких-нибудь сайтов (лучше - с помощью scrapy или какой-то другой библиотеки, предназначенной для краулинга, но можно с помощью requests и любой библиотеки для парсинга html-кода).
